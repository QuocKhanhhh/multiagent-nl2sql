import json
import logging
import re
from typing import List, Tuple, Dict, Any

import requests
from requests.exceptions import ReadTimeout, RequestException

logger = logging.getLogger("analytics.nl2sql_generator")

# =========================
# Config
# =========================
OLLAMA_HOST = "http://host.docker.internal:11434"
OLLAMA_TIMEOUT = 60
MAX_RETRIES = 2

def intelligent_join_builder(plan: dict) -> str:
    all_columns_text = json.dumps(plan)
    used_aliases = set(re.findall(r'\b(fa|da|au|dt|dd)\b', all_columns_text))

    if "fa" not in used_aliases or len(used_aliases) <= 1:
        return "FROM dw.fact_articles fa"

    joins = ["FROM dw.fact_articles fa"]
    join_map = {
        "da": "INNER JOIN dw.dim_articles da ON fa.article_id = da.article_id",
        "au": "INNER JOIN dw.dim_authors au ON fa.author_id = au.author_id",
        "dt": "INNER JOIN dw.dim_topics dt ON fa.topic_id = dt.topic_id",
        "dd": "INNER JOIN dw.dim_date dd ON fa.date_id = dd.date_id",
    }

    for alias in ["da", "au", "dt", "dd"]:
        if alias in used_aliases:
            joins.append(join_map[alias])

    return "\n".join(joins)

# ----- New helpers: schema index & fuzzy column matcher -----
def build_schema_index(catalog: dict) -> Dict[str, set]:
    idx = {}
    for t in catalog.get("tables", []):
        # Äáº£m báº£o chá»‰ láº¥y tÃªn cá»™t tá»« key 'name'
        cols = {c['name'] for c in t.get("columns", []) if isinstance(c, dict) and 'name' in c}
        idx[t["name"]] = cols
    return idx

def normalize_col_name(s: str) -> str:
    if not s:
        return ""
    return re.sub(r'[^a-z0-9]', '', s.lower())

def find_best_column_match(table: str, requested: str, schema_index: Dict[str, set]) -> str | None:
    if table not in schema_index:
        return None
    cols = schema_index[table]
    if requested in cols:
        return requested
    req_norm = normalize_col_name(requested)
    for c in cols:
        if normalize_col_name(c) == req_norm:
            return c
    for c in cols:
        cn = normalize_col_name(c)
        if req_norm in cn or cn in req_norm:
            return c
    for c in cols:
        cn = normalize_col_name(c)
        if cn.startswith(req_norm) or req_norm.startswith(cn):
            return c
    return None

def filters_to_sql_where(filters: List[Dict[str, Any]]) -> str:
    """
    Chuyá»ƒn filters tá»« object thÃ nh chuá»—i Ä‘iá»u kiá»‡n SQL há»£p lá»‡.
    Há»— trá»£ toÃ¡n tá»­ IN vá»›i giÃ¡ trá»‹ lÃ  list.
    """
    conditions = []
    for f in filters:
        col = f.get("column", "")
        op = f.get("operator", "=").upper()
        val = f.get("value")

        if not col or val is None:
            continue

        if op == "IN" and isinstance(val, list):
            # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho toÃ¡n tá»­ IN vá»›i list
            if not val: continue # Bá» qua náº¿u list rá»—ng
            # Chuyá»ƒn Ä‘á»•i cÃ¡c pháº§n tá»­ trong list thÃ nh chuá»—i cÃ³ dáº¥u nhÃ¡y Ä‘Æ¡n
            formatted_vals = [f"'{str(v).strip()}'" for v in val]
            conditions.append(f"{col} IN ({', '.join(formatted_vals)})")
        else:
            # Xá»­ lÃ½ nhÆ° cÅ© cho cÃ¡c trÆ°á»ng há»£p khÃ¡c
            if isinstance(val, str):
                val = val.strip()
                if not (val.startswith("'") and val.endswith("'")):
                    val = f"'{val}'"
            else:
                val = str(val)
            conditions.append(f"{col} {op} {val}")

    return " AND ".join(conditions) if conditions else ""

def conditions_to_sql(conditions: List[Dict[str, Any]]) -> str:
    # Äá»•i tÃªn hÃ m cÅ© Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng
    return filters_to_sql_where(conditions)

# =========================
# Prompt cho cÃ¡c agent
# =========================
# Sá»­a PROMPT_DECONSTRUCTOR

# Cáº­p nháº­t PROMPT_DECONSTRUCTOR
PROMPT_DECONSTRUCTOR = """
Báº¡n lÃ  Deconstructor Agent. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  phÃ¢n tÃ­ch cÃ¢u há»i tiáº¿ng Viá»‡t vÃ  chuyá»ƒn thÃ nh logical plan JSON.

YÃŠU Cáº¦U:
- metric: kiá»ƒu phÃ©p tÃ­nh ("count", "avg", "min", "max", "sum"...)
- metric_hint: mÃ´ táº£ metric báº±ng tiáº¿ng Viá»‡t
- dimensions: danh sÃ¡ch cÃ¡c cá»™t há»£p lá»‡ Ä‘á»ƒ group by.
- filters: danh sÃ¡ch cÃ¡c Ä‘iá»u kiá»‡n lá»c. Má»—i Ä‘iá»u kiá»‡n LÃ€ Má»˜T OBJECT cÃ³ dáº¡ng {"column": "...", "operator": "=", "value": "..."}.
- order_by: {"column": "...", "direction": "ASC|DESC"}.
- limit: sá»‘ nguyÃªn.
- from_tables: LUÃ”N chá»©a ["dw.fact_articles"].
- aliases: LUÃ”N khai bÃ¡o Ã­t nháº¥t {"fa": "dw.fact_articles"}.
  Náº¿u dÃ¹ng cá»™t tá»« dim_articles, dim_authors, dim_topics, dim_date thÃ¬ thÃªm tÆ°Æ¡ng á»©ng:
  {"da": "dw.dim_articles"}, {"au": "dw.dim_authors"}, {"dt": "dw.dim_topics"}, {"dd": "dw.dim_date"}.
- filters: danh sÃ¡ch **Táº¤T Cáº¢** cÃ¡c Ä‘iá»u kiá»‡n lá»c (WHERE). Má»—i Ä‘iá»u kiá»‡n lÃ  má»™t object.
- having: dÃ¹ng cho Ä‘iá»u kiá»‡n trÃªn cÃ¡c cá»™t Ä‘Ã£ gá»™p nhÃ³m (HAVING, vÃ­ dá»¥: COUNT(*) > 100).

HÆ¯á»šNG DáºªN Vá»Š TRÃ Cá»˜T (QUAN TRá»ŒNG):
- CÃ¡c cá»™t mÃ´ táº£ ná»™i dung bÃ i viáº¿t náº±m á»Ÿ `da` (dim_articles): da.title, da.source_name, da.content.
- CÃ¡c cá»™t mÃ´ táº£ tÃ¡c giáº£ náº±m á»Ÿ `au` (dim_authors): au.author_name.
- CÃ¡c cá»™t mÃ´ táº£ chá»§ Ä‘á» náº±m á»Ÿ `dt` (dim_topics): dt.topic_name.
- CÃ¡c cá»™t mÃ´ táº£ ngÃ y thÃ¡ng náº±m á»Ÿ `dd` (dim_date): dd.year, dd.month, dd.day.
- CÃ¡c chá»‰ sá»‘ Ä‘o lÆ°á»ng náº±m á»Ÿ `fa` (fact_articles): fa.word_count, fa.read_time, fa.sentiment.

QUY Táº®C QUAN TRá»ŒNG:
1. Chá»‰ dÃ¹ng cÃ¡c báº£ng/alias: fa, da, au, dt, dd.
2. Vá»›i cÃ¢u há»i vá» "cao nháº¥t", "tháº¥p nháº¥t", "nhiá»u nháº¥t", "Ã­t nháº¥t":
   - Báº®T BUá»˜C dÃ¹ng order_by + limit: 1.
   - KHÃ”NG Ä‘Æ°á»£c táº¡o filter so sÃ¡nh trá»±c tiáº¿p vá»›i giÃ¡ trá»‹ lá»›n nháº¥t/nhá» nháº¥t.
3. Náº¿u liÃªn quan Ä‘áº¿n cáº£m xÃºc "tÃ­ch cá»±c", filter lÃ : [{"column": "fa.sentiment", "operator": "=", "value": "pos"}].
4. Náº¿u liÃªn quan Ä‘áº¿n "chá»§ Ä‘á»", hÃ£y group by dt.topic_name (khÃ´ng dÃ¹ng dt.topic_id).
5. Náº¿u liÃªn quan Ä‘áº¿n "tÃ¡c giáº£", hÃ£y group by au.author_name (khÃ´ng dÃ¹ng au.author_id).
6. Tuyá»‡t Ä‘á»‘i KHÃ”NG thÃªm báº¥t ká»³ Ä‘iá»u kiá»‡n nÃ o vÃ o "filters" náº¿u cÃ¢u há»i khÃ´ng yÃªu cáº§u rÃµ rÃ ng. VÃ­ dá»¥: cÃ¢u há»i "chá»§ Ä‘á» cÃ³ sá»‘ tá»« cao nháº¥t" thÃ¬ "filters" pháº£i lÃ  [].
7. Khi cÃ¢u há»i nháº¯c Ä‘áº¿n "thá»i gian Ä‘á»c", Báº®T BUá»˜C dÃ¹ng cá»™t `fa.read_time`, khÃ´ng dÃ¹ng tÃªn nÃ o khÃ¡c.
8. Náº¿u liÃªn quan Ä‘áº¿n cáº£m xÃºc "tÃ­ch cá»±c", filter lÃ : [{"column": "fa.sentiment", "operator": "=", "value": "pos"}].
9.  Gá»™p Táº¤T Cáº¢ cÃ¡c Ä‘iá»u kiá»‡n lá»c vÃ o chung má»™t danh sÃ¡ch "filters". VÃ­ dá»¥: lá»c theo sentiment VÃ€ nÄƒm, thÃ¬ "filters" sáº½ lÃ  má»™t danh sÃ¡ch chá»©a hai object.
10. Sá»­ dá»¥ng "having" cho cÃ¡c Ä‘iá»u kiá»‡n lá»c sau khi Ä‘Ã£ GROUP BY. VÃ­ dá»¥: "chá»§ Ä‘á» cÃ³ Ã­t nháº¥t 100 bÃ i viáº¿t" -> `GROUP BY dt.topic_name`, `having: [{"column": "COUNT(fa.article_id)", "operator": ">=", "value": 100}]`.
11. KHÃ”NG THÃŠM `filters` Náº¾U CÃ‚U Há»ŽI KHÃ”NG YÃŠU Cáº¦U.
12. Khi há»i vá» "cao nháº¥t", "tá»•ng", ... khÃ´ng cÃ³ nghÄ©a lÃ  pháº£i lá»c theo "tÃ­ch cá»±c". `filters` pháº£i lÃ  [].

QUY Táº®C VÃ€NG (Báº®T BUá»˜C TUÃ‚N THá»¦):
- ðŸ›‘ **SELECT CHá»ˆ CÃC Cá»˜T TRONG `dimensions` VÃ€ `metric`:** CÃ¢u lá»‡nh SELECT chá»‰ Ä‘Æ°á»£c chá»©a cÃ¡c cá»™t trong `dimensions` vÃ  phÃ©p tÃ­nh `metric`. KHÃ”NG thÃªm cÃ¡c cá»™t khÃ¡c.
- ðŸ›‘ **GROUP BY CHá»ˆ CÃC Cá»˜T TRONG `dimensions`:** Má»‡nh Ä‘á» GROUP BY pháº£i chá»©a Táº¤T Cáº¢ vÃ  CHá»ˆ cÃ¡c cá»™t trong `dimensions`.
- ðŸ›‘ **Äá»ŒC Ká»¸ CÃ‚U Há»ŽI Äá»‚ XÃC Äá»ŠNH `dimensions`:** Náº¿u cÃ¢u há»i lÃ  "TÃ¡c giáº£ nÃ o...", thÃ¬ `dimensions` pháº£i lÃ  `[au.author_name]`. Náº¿u cÃ¢u há»i lÃ  "NgÃ y nÃ o...", thÃ¬ `dimensions` pháº£i lÃ  `[dd.full_date]`.
- ðŸ›‘ **Lá»c ngÃ y Ä‘áº§y Ä‘á»§:** Khi cÃ¢u há»i cÃ³ ngÃ y cá»¥ thá»ƒ (vÃ­ dá»¥: "15/6/2022"), hÃ£y lá»c theo cáº£ 3 cá»™t: `dd.day=15`, `dd.month=6`, `dd.year=2022`.
- ðŸ›‘ **Hiá»ƒu "A so vá»›i B":** Khi so sÃ¡nh (vÃ­ dá»¥: "nÄƒm 2019 so vá»›i 2020"), hÃ£y dÃ¹ng toÃ¡n tá»­ `IN` cho `filters`, vÃ­ dá»¥: `{"column": "dd.year", "operator": "IN", "value": [2019, 2020]}`.
- ðŸ›‘ **Hiá»ƒu "...nháº¥t":** Khi há»i "Ai/CÃ¡i gÃ¬ ... nháº¥t" (vÃ­ dá»¥: "tiÃªu cá»±c nháº¥t"), hÃ£y hiá»ƒu lÃ  Ä‘áº¿m sá»‘ lÆ°á»£ng (`COUNT`) vÃ  sáº¯p xáº¿p giáº£m dáº§n (`DESC`), khÃ´ng pháº£i láº¥y `MAX` cá»§a má»™t cá»™t khÃ¡c.
- ðŸ›‘ **Chá»n Ä‘Ãºng phÃ©p tÃ­nh:** "Tá»•ng tháº¥p nháº¥t" nghÄ©a lÃ  tÃ­nh `SUM` rá»“i `ORDER BY ... ASC`. "GiÃ¡ trá»‹ tháº¥p nháº¥t" má»›i lÃ  dÃ¹ng `MIN`. TÆ°Æ¡ng tá»± vá»›i "cao nháº¥t".
- ðŸ›‘ **Äáº¿m sá»‘ lÆ°á»£ng Ä‘á»‘i tÆ°á»£ng:** Náº¿u cÃ¢u há»i lÃ  "CÃ³ bao nhiÃªu tÃ¡c giáº£/chá»§ Ä‘á»...", `metric` pháº£i lÃ  `COUNT(DISTINCT ...)`, vÃ  `dimensions` pháº£i Ä‘á»ƒ trá»‘ng `[]`.

VÃ Dá»¤ 1:
CÃ¢u há»i: "Nguá»“n nÃ o cÃ³ trung bÃ¬nh sá»‘ tá»« bÃ i viáº¿t tháº¥p nháº¥t?"
Plan JSON:
{
  "from_tables": ["dw.fact_articles"],
  "aliases": {"fa": "dw.fact_articles", "da": "dw.dim_articles"},
  "metric": "avg",
  "metric_hint": "Trung bÃ¬nh sá»‘ tá»« theo nguá»“n",
  "dimensions": ["da.source_name"],
  "filters": [],
  "order_by": {"column": "avg(fa.word_count)", "direction": "ASC"},
  "limit": 1
}

VÃ Dá»¤ 2:
CÃ¢u há»i: "CÃ³ bao nhiÃªu bÃ i viáº¿t vá» chá»§ Ä‘á» 'the-thao'?"
Plan JSON:
{
  "from_tables": ["dw.fact_articles"],
  "aliases": {"fa": "dw.fact_articles", "dt": "dw.dim_topics"},
  "metric": "count",
  "metric_hint": "Sá»‘ bÃ i viáº¿t vá» thá»ƒ thao",
  "dimensions": [],
  "filters": [{"column": "dt.topic_name", "operator": "=", "value": "the-thao"}],
  "order_by": {},
  "limit": null
}

VÃ Dá»¤ 3:
CÃ¢u há»i: "Chá»§ Ä‘á» nÃ o cÃ³ hÆ¡n 500 bÃ i viáº¿t?"
Plan JSON:
{
  "from_tables": ["dw.fact_articles", "dw.dim_topics"],
  "aliases": {"fa": "dw.fact_articles", "dt": "dw.dim_topics"},
  "metric": "count",
  "metric_hint": "Sá»‘ bÃ i viáº¿t theo chá»§ Ä‘á»",
  "dimensions": ["dt.topic_name"],
  "filters": [],
  "having": [{"column": "COUNT(fa.article_id)", "operator": ">", "value": 500}],
  "order_by": {},
  "limit": null
}
"""

# Sá»­a PROMPT_PLANNER

PROMPT_PLANNER = """
Báº¡n lÃ  Planner Agent. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  chuyá»ƒn logical plan JSON thÃ nh SQL há»£p lá»‡.

QUY Táº®C:
- Chá»‰ dÃ¹ng cÃ¡c báº£ng/alias: fa (dw.fact_articles), da (dw.dim_articles), au (dw.dim_authors), dt (dw.dim_topics), dd (dw.dim_date).
- JOIN Ä‘Ãºng khÃ³a ngoáº¡i:
  - fa.article_id = da.article_id
  - fa.author_id = au.author_id
  - fa.topic_id = dt.topic_id
  - fa.date_id = dd.date_id
- Dá»±a vÃ o cÃ¡c cá»™t trong 'dimensions', 'filters', 'order_by' Ä‘á»ƒ quyáº¿t Ä‘á»‹nh JOIN báº£ng nÃ o.
- Náº¿u cÃ³ 'order_by', hÃ£y thÃªm má»‡nh Ä‘á» ORDER BY.
- Náº¿u cÃ³ 'limit', hÃ£y thÃªm má»‡nh Ä‘á» LIMIT.
- Náº¿u cÃ³ 'dimensions' vÃ  'metric' (count, sum, avg...), hÃ£y dÃ¹ng GROUP BY cho táº¥t cáº£ cÃ¡c cá»™t trong 'dimensions'.
- KHÃ”NG bao giá» dÃ¹ng `fa.source_id`. Náº¿u plan yÃªu cáº§u liÃªn quan Ä‘áº¿n "nguá»“n", hÃ£y group by hoáº·c filter báº±ng **da.source_name**.
- Chá»‰ tráº£ vá» SQL thuáº§n, báº¯t Ä‘áº§u báº±ng SELECT, khÃ´ng giáº£i thÃ­ch.
- KHÃ”NG tá»± sinh Ä‘iá»u kiá»‡n `IS NOT NULL` hoáº·c cá»™t lá»c ngoÃ i nhá»¯ng gÃ¬ cÃ³ trong plan.filters.

OUTPUT: chá»‰ SQL statement.
"""

# =========================
# Ollama query wrapper
# =========================
def query_ollama(model: str, role: str, user_input: str, expect_json: bool = True) -> dict | str:
    valid_roles = {"deconstructor", "planner", "corrector"}
    if role not in valid_roles:
        raise ValueError(f"Unknown role {role}")

    # Chá»n prompt dá»±a role (giá»¯ nguyÃªn hoáº·c tuá»³ chá»‰nh náº¿u cáº§n)
    if role == "deconstructor":
        system_prompt = PROMPT_DECONSTRUCTOR
    elif role == "planner":
        system_prompt = PROMPT_PLANNER
    else:
        system_prompt = ""

    payload = {
        "model": model,
        "options": {"temperature": 0.0},
        "prompt": f"{system_prompt.strip()}\n\nCÃ¢u há»i hoáº·c plan:\n{user_input}\n\nTráº£ lá»i:",
        "stream": False,
    }
    url = f"{OLLAMA_HOST}/api/generate"
    last_err = None

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = requests.post(url, json=payload, timeout=OLLAMA_TIMEOUT)
            resp.raise_for_status()
            resp_json = resp.json()
            raw_text = resp_json.get("response", "").strip()
            logger.info("Raw Ollama response (%s, attempt %d): %s", role, attempt, raw_text[:500])
            if expect_json:
                # Biá»ƒu thá»©c chÃ­nh quy má»›i: tÃ¬m khá»‘i JSON náº±m giá»¯a ```json vÃ  ``` hoáº·c chá»‰ ``` vÃ  ```
                match = re.search(r"```(?:json)?\s*({[\s\S]*?})\s*```", raw_text)
                candidate = ""
                if match:
                    candidate = match.group(1).strip()
                else:
                    # Fallback: náº¿u khÃ´ng cÃ³ ```, thá»­ tÃ¬m JSON Ä‘áº§u tiÃªn trong chuá»—i
                    start_index = raw_text.find('{')
                    if start_index != -1:
                        # TÃ¬m dáº¥u ngoáº·c nhá»n Ä‘Ã³ng tÆ°Æ¡ng á»©ng
                        brace_count = 0
                        json_end = -1
                        for i, char in enumerate(raw_text[start_index:]):
                            if char == '{':
                                brace_count += 1
                            elif char == '}':
                                brace_count -= 1
                                if brace_count == 0:
                                    json_end = start_index + i + 1
                                    break
                        if json_end != -1:
                            candidate = raw_text[start_index:json_end]

                if candidate:
                    try:
                        parsed = json.loads(candidate)
                        return parsed
                    except json.JSONDecodeError as e:
                        logger.error("JSON parse failed after cleaning: %s. Raw candidate: %s", str(e), candidate[:300])
                        return {"error": "failed_parse", "raw": raw_text}
                else:
                    logger.error("Could not extract any JSON from raw response. Raw: %s", raw_text[:300])
                    return {"error": "no_json_found", "raw": raw_text}
            else:
                return raw_text
        except ReadTimeout as e:
            last_err = e
            logger.warning("Ollama %s timeout (attempt %d/%d)", role, attempt, MAX_RETRIES)
        except RequestException as e:
            last_err = e
            logger.error("Ollama %s request error: %s", role, str(e))
            break

    return {"error": "request_failed", "detail": str(last_err)}


# =========================
# Schema Validation Agent
# =========================
def schema_validation_agent(plan: dict, catalog: dict) -> Tuple[bool, List[str]]:
    errors: List[str] = []
    if not plan or not isinstance(plan, dict):
        return False, ["Plan is empty or not dict."]

    # XÃ¢y dá»±ng index schema má»™t cÃ¡ch chÃ­nh xÃ¡c
    valid_columns_per_table = build_schema_index(catalog)
    valid_tables = set(valid_columns_per_table.keys())

    alias_to_table: Dict[str, str] = plan.get("aliases", {})

    # Kiá»ƒm tra xem alias cÃ³ trá» Ä‘áº¿n báº£ng há»£p lá»‡ khÃ´ng
    for alias, table_name in alias_to_table.items():
        if table_name not in valid_tables:
            errors.append(f"Alias '{alias}' points to an invalid table '{table_name}'")

    def check_expr(expr: str):
        if not expr: return
        # TÃ¬m cÃ¡c cáº·p alias.column trong biá»ƒu thá»©c
        found = re.findall(r'\b([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)\b', str(expr))
        for alias, column in found:
            if alias in alias_to_table:
                table_name = alias_to_table[alias]
                if table_name in valid_columns_per_table and column not in valid_columns_per_table[table_name]:
                    errors.append(f"Invalid column '{column}' in '{table_name}' (alias '{alias}')")
            else:
                errors.append(f"Undefined alias '{alias}' used for column '{column}'")

    # Kiá»ƒm tra cÃ¡c trÆ°á»ng trong plan
    for col in plan.get("dimensions", []):
        check_expr(str(col))

    for f in plan.get("filters", []):
        if isinstance(f, dict):
            check_expr(f.get("column", ""))

    order_by = plan.get("order_by") or {}
    if isinstance(order_by, dict) and order_by.get("column"):
        check_expr(order_by["column"])

    # Kiá»ƒm tra metric_col náº¿u cÃ³
    check_expr(plan.get("metric_col"))

    return (len(errors) == 0), list(set(errors))

# =========================
# Normalize Plan
# =========================
ALIAS_FIX_MAP = {
    "fact_articles": "dw.fact_articles",
    "articles": "dw.dim_articles",
    "authors": "dw.dim_authors",
    "topics": "dw.dim_topics",
    "date": "dw.dim_date",
}

def normalize_plan(plan: dict, valid_tables: set, schema: dict) -> dict:
    if not plan or not isinstance(plan, dict):
        return plan

    schema_index = build_schema_index(schema)

    aliases = plan.get("aliases") or {}
    new_aliases = {}

    # Chuyá»ƒn alias báº£ng thÃ nh tÃªn báº£ng chuáº©n xÃ¡c
    for alias, table in aliases.items():
        if table in valid_tables:
            new_aliases[alias] = table
        else:
            token = str(table).split('.')[-1]
            matched = None
            for t in schema_index.keys():
                if t.endswith(token) or normalize_col_name(t).endswith(normalize_col_name(token)):
                    matched = t
                    break
            new_aliases[alias] = matched or table
    plan["aliases"] = new_aliases

    # Ãnh xáº¡ metric_hint sang cá»™t thá»±c táº¿
    metric_hint = plan.get("metric_hint", "").lower()
    if "sá»‘ tá»«" in metric_hint or "word_count" in metric_hint:
        plan["metric_col"] = "fa.word_count"
    elif "thá»i gian Ä‘á»c" in metric_hint or "read_time" in metric_hint:
        plan["metric_col"] = "fa.read_time"
    else:
        plan["metric_col"] = None

    # Chá»‰nh sá»­a order_by column náº¿u cÃ³ metric_col + metric aggregation
    if plan.get("order_by") and plan["order_by"]:
        ob = plan["order_by"]
        col = ob.get("column")
        metric = plan.get("metric")
        if col and metric and metric in ("sum", "avg", "max", "min") and plan.get("metric_col"):
            plan["order_by"]["column"] = f"{metric.upper()}({plan['metric_col']})"

    # Chuyá»ƒn filters thÃ nh Ä‘iá»u kiá»‡n WHERE há»£p lá»‡
    filters_raw = plan.get("filters", [])
    if filters_raw and isinstance(filters_raw, list) and filters_raw and isinstance(filters_raw[0], dict):
        where_clause = filters_to_sql_where(filters_raw)
        plan["where_conditions"] = [where_clause] if where_clause else []
    else:
        plan["where_conditions"] = filters_raw

    # Chuáº©n alias.column há»£p lá»‡ trong cÃ¡c trÆ°á»ng: select_columns, group_by, where_conditions, having, order_by...
    def resolve_expr(expr: str) -> str:
        if not expr or not isinstance(expr, str):
            return expr
        expr = expr.strip()
        def replace_match(m):
            alias, col = m.group(1), m.group(2)
            alias_table = plan.get("aliases", {}).get(alias) or alias
            table_name = alias_table if alias_table in schema_index else None
            if not table_name:
                for t in plan.get("from_tables", []):
                    if t.endswith(alias) or normalize_col_name(t).endswith(normalize_col_name(alias)):
                        table_name = t
                        break
            table_name = table_name or alias_table

            # Thay tháº¿ _id báº±ng _name náº¿u phÃ¹ há»£p
            if col.endswith("_id"):
                prefix = col[:-3]
                candidate_name = f"{prefix}_name"
                best = find_best_column_match(table_name, candidate_name, schema_index)
                if best:
                    return f"{alias}.{best}"
            best = find_best_column_match(table_name, col, schema_index)
            if best:
                return f"{alias}.{best}"
            return f"{alias}.{col}"

        out = re.sub(r'([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)', replace_match, expr)
        return out.strip()

    # Chuáº©n hÃ³a cÃ¡c trÆ°á»ng trong plan
    if "select_columns" in plan:
        sel = []
        for col in plan["select_columns"]:
            if isinstance(col, dict):
                expr = col.get("expr", "")
                fixed = resolve_expr(expr)
                sel.append({"expr": fixed, "alias": col.get("alias")})
            else:
                fixed = resolve_expr(col)
                sel.append(fixed)
        plan["select_columns"] = sel

    plan["group_by"] = [resolve_expr(g) for g in (plan.get("group_by") or [])]
    plan["where_conditions"] = [resolve_expr(w) for w in (plan.get("where_conditions") or [])]
    plan["having"] = [resolve_expr(h) for h in (plan.get("having") or [])]

    if plan.get("order_by"):
        ob = plan["order_by"]
        if isinstance(ob, dict):
            col = resolve_expr(ob.get("column", ""))
            if col:
                plan["order_by"]["column"] = col
            else:
                plan["order_by"] = {}

    new_from = []
    for t in plan.get("from_tables") or []:
        if t in schema_index:
            new_from.append(t)
            continue
        token = str(t).split('.')[-1]
        matched = None
        for st in schema_index.keys():
            if st.endswith(token) or normalize_col_name(st).endswith(normalize_col_name(token)):
                matched = st
                break
        new_from.append(matched or t)
    plan["from_tables"] = new_from

    return plan

def postprocess_sql(sql: str) -> str:
    if not sql or not isinstance(sql, str):
        return sql
    txt = sql.strip()
    txt = re.sub(r"```$", "", txt).strip("`\n\r ")
    txt = re.sub(r'ORDER\s+BY\s*;', ';', txt, flags=re.IGNORECASE)
    txt = re.sub(r'ORDER\s+BY\s*(LIMIT|$)', r'\1', txt, flags=re.IGNORECASE)
    txt = re.sub(r'\$([a-zA-Z0-9_]+\.[a-zA-Z0-9_]+)', r'\1', txt)
    txt = re.sub(r';{2,}', ';', txt)
    if not txt.endswith(';'):
        txt += ';'
    return txt

# =========================
# Agents wrapper
# =========================
def query_deconstructor_agent(question: str) -> dict:
    return query_ollama("mistral:7b", "deconstructor", question, expect_json=True)

def query_planner_agent(plan_json: Any, schema: dict = None) -> str:
    plan_dict = json.loads(plan_json) if isinstance(plan_json, str) else plan_json
    join_clause = intelligent_join_builder(plan_dict)

    select_clause = []
    metric = plan_dict.get("metric")
    metric_col = plan_dict.get("metric_col")
    dimensions = plan_dict.get("dimensions", [])

    # LuÃ´n thÃªm dimensions vÃ o SELECT náº¿u cÃ³
    if dimensions:
        select_clause.extend(dimensions)

    # ThÃªm metric vÃ o SELECT
    if metric == "count":
        # Náº¿u chá»‰ cÃ³ metric count, khÃ´ng cÃ³ dimension, thÃ¬ chá»‰ cáº§n COUNT(*)
        if not dimensions:
            select_clause = ["COUNT(*) AS count_result"]
        else: # Náº¿u cÃ³ dimension, thÃªm COUNT(*) bÃªn cáº¡nh
            select_clause.append("COUNT(*) AS count_result")
    elif metric and metric_col:
        select_clause.append(f"{metric.upper()}({metric_col}) AS {metric}_result")

    # Fallback: náº¿u select clause váº«n rá»—ng, máº·c Ä‘á»‹nh lÃ  COUNT(*)
    if not select_clause:
        select_clause.append("COUNT(*) AS count_result")

    # GhÃ©p láº¡i thÃ nh cÃ¢u SQL
    sql = f"SELECT {', '.join(select_clause)}\n{join_clause}"

    filters = plan_dict.get("where_conditions", [])
    # Äáº£m báº£o filters khÃ´ng rá»—ng vÃ  pháº§n tá»­ Ä‘áº§u tiÃªn khÃ´ng rá»—ng
    if filters and filters[0]:
        sql += f"\nWHERE {filters[0]}"

    if dimensions:
        sql += f"\nGROUP BY {', '.join(dimensions)}"
        
    having_conditions_raw = plan_dict.get("having", [])    
    if having_conditions_raw:
        having_clause = conditions_to_sql(having_conditions_raw)
        if having_clause:
            sql += f"\nHAVING {having_clause}"

    if plan_dict.get("order_by"):
        ob = plan_dict["order_by"]
        if isinstance(ob, dict) and ob.get("column"):
            sql += f"\nORDER BY {ob['column']} {ob.get('direction', 'ASC')}"

    if plan_dict.get("limit"):
        sql += f"\nLIMIT {plan_dict['limit']}"

    return postprocess_sql(sql)

# =========================
# Pipeline
# =========================
def multi_agent_pipeline(question: str, schema: dict = None) -> Tuple[str, List[str], dict]:
    # Step 1: Deconstructor
    decon = query_deconstructor_agent(question)
    if "error" in decon:
        return "-- PLAN_VALIDATION_ERROR: deconstructor_failed", [decon["error"]], None

    # Step 2: Normalize - pass schema along
    if schema:
        decon = normalize_plan(decon, {t["name"] for t in schema.get("tables", [])}, schema)

    # Step 3: Validation
    if schema:
        is_valid, plan_errors = schema_validation_agent(decon, schema)
        if not is_valid:
            return f"-- PLAN_VALIDATION_ERROR: Schema validation failed -> {'; '.join(plan_errors)}", plan_errors, decon

    # Step 4: Planner â†’ SQL (give schema so postprocessing can be smarter if needed)
    sql_out = query_planner_agent(decon, schema=schema)
    if not isinstance(sql_out, str):
        return "-- PLAN_VALIDATION_ERROR: planner_failed", ["planner_failed"], decon

    return sql_out, [], decon


    return sql_out, [], decon

def corrector_agent(sql: str, error: str, schema_text: str, question: str, plan: dict) -> str | dict:
    prompt = f"""
Báº¡n lÃ  chuyÃªn gia sá»­a SQL PostgreSQL. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  sá»­a cÃ¢u SQL bá»‹ lá»—i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: "{question}"
Schema cÃ¡c báº£ng: {schema_text}
Logical Plan (Ã½ Ä‘á»‹nh ban Ä‘áº§u): {json.dumps(plan, ensure_ascii=False, indent=2)}

SQL bá»‹ lá»—i:
{sql}
Lá»—i tá»« database hoáº·c validator: "{error}"

PhÃ¢n tÃ­ch lá»—i vÃ  yÃªu cáº§u sá»­a:

Äá»c ká»¹ lá»—i: Lá»—i nÃ y cÃ³ thá»ƒ lÃ  do cá»™t khÃ´ng tá»“n táº¡i, sai tÃªn báº£ng, hoáº·c sai logic GROUP BY.

Äá»‘i chiáº¿u vá»›i Plan vÃ  Schema: Kiá»ƒm tra xem SQL cÃ³ tuÃ¢n thá»§ Ä‘Ãºng cÃ¡c cá»™t trong schema vÃ  Ã½ Ä‘á»‹nh trong plan khÃ´ng. VÃ­ dá»¥: plan yÃªu cáº§u 'group by' cá»™t A, nhÆ°ng SQL láº¡i thiáº¿u.

Sá»­a SQL: Viáº¿t láº¡i cÃ¢u lá»‡nh SQL SELECT cho Ä‘Ãºng. Äáº£m báº£o nÃ³ tráº£ lá»i Ä‘Æ°á»£c cÃ¢u há»i ban Ä‘áº§u.

Chá»‰ tráº£ vá» SQL: Káº¿t quáº£ cuá»‘i cÃ¹ng chá»‰ chá»©a mÃ£ SQL, káº¿t thÃºc báº±ng dáº¥u cháº¥m pháº©y, khÃ´ng cÃ³ giáº£i thÃ­ch hay ```sql.

VÃ­ dá»¥ sá»­a lá»—i 'Non-aggregated select column not in GROUP BY':

Lá»—i: Cá»™t 'da.source_name' pháº£i xuáº¥t hiá»‡n trong má»‡nh Ä‘á» GROUP BY hoáº·c Ä‘Æ°á»£c sá»­ dá»¥ng trong má»™t hÃ m tá»•ng há»£p.

Sá»­a: ThÃªm 'da.source_name' vÃ o má»‡nh Ä‘á» GROUP BY.

BÃ¢y giá», hÃ£y sá»­a cÃ¢u SQL trÃªn.
"""
    # use simple prompt mode: pass prompt text and request parsed JSON only if the model returns JSON
    resp = query_ollama(prompt, model="mistral:7b", expect_json=False)
    # if model returned a JSON-like dict (rare here), return it as-is
    if isinstance(resp, dict):
        return resp
    # else resp is text; extract SQL if possible
    txt = str(resp).strip()
    txt = re.sub(r"^```(?:sql)?", "", txt, flags=re.IGNORECASE).strip("`\n ")
    m = re.search(r"(SELECT[\s\S]*?;)", txt, re.IGNORECASE)
    if m:
        return m.group(1).strip()
    if txt.upper().startswith("SELECT"):
        return txt
    # fallback
    return {"error": "cannot_fix", "reason": "LLM did not produce valid SQL"}

def preprocess_question(q: str) -> str:
    return q.strip()